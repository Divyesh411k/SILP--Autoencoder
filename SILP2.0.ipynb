{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir_occluded1, root_dir_occluded2, root_dir_clear, transform=None):\n",
    "        self.root_dir_occluded1 = root_dir_occluded1\n",
    "        self.root_dir_occluded2 = root_dir_occluded2\n",
    "        self.root_dir_clear = root_dir_clear\n",
    "        self.transform = transform\n",
    "\n",
    "        # Collect all image paths from nested directories for occluded images 1\n",
    "        self.occluded1_image_paths = []\n",
    "        for root, _, files in os.walk(root_dir_occluded1):\n",
    "            for file in files:\n",
    "                if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "                    self.occluded1_image_paths.append(os.path.join(root, file))\n",
    "\n",
    "        # Collect all image paths from nested directories for occluded images 2\n",
    "        self.occluded2_image_paths = []\n",
    "        for root, _, files in os.walk(root_dir_occluded2):\n",
    "            for file in files:\n",
    "                if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "                    self.occluded2_image_paths.append(os.path.join(root, file))\n",
    "\n",
    "        # Collect all image paths from nested directories for clear images\n",
    "        self.clear_image_paths = []\n",
    "        for root, _, files in os.walk(root_dir_clear):\n",
    "            for file in files:\n",
    "                if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "                    self.clear_image_paths.append(os.path.join(root, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.occluded1_image_paths) + len(self.occluded2_image_paths) + len(self.clear_image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.occluded1_image_paths):\n",
    "            img_name = self.occluded1_image_paths[idx]\n",
    "            label = 0  \n",
    "        elif idx < len(self.occluded1_image_paths) + len(self.occluded2_image_paths):\n",
    "            img_name = self.occluded2_image_paths[idx - len(self.occluded1_image_paths)]\n",
    "            label = 0  \n",
    "        else:\n",
    "            img_name = self.clear_image_paths[idx - len(self.occluded1_image_paths) - len(self.occluded2_image_paths)]\n",
    "            label = 1 \n",
    "        \n",
    "        image = Image.open(img_name)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "shuffle=True\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "])\n",
    "root_dir_occluded1=r'masked'\n",
    "root_dir_occluded2=r'sunglasses'\n",
    "root_dir_clear=r'neutral'\n",
    "my_dataset = CustomDataset(root_dir_occluded1, root_dir_occluded2, root_dir_clear, transform=transform)\n",
    "data_loader = DataLoader(my_dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def pixel_loss(output, target):\n",
    "    c, h, w = target.size(1), target.size(2), target.size(3)\n",
    "    loss = nn.MSELoss(reduction='sum')(output, target) / (c * h * w)\n",
    "    return loss\n",
    "\n",
    "def perceptual_loss(output, target, vgg_model):\n",
    "\n",
    "    output_features = vgg_model(output)\n",
    "    target_features = vgg_model(target)\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(len(output_features)):\n",
    "        for j in range(len(output_features[i])):\n",
    "            c_i_j, h_i_j, w_i_j = output_features[i][j].size(1), output_features[i][j].size(2), output_features[i][j].size(3)\n",
    "            loss += nn.L1Loss(reduction='sum')(output_features[i][j], target_features[i][j]) / (c_i_j * h_i_j * w_i_j)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def lr_average_loss(output, target):\n",
    "    \n",
    "    c, h, w = target.size(1), target.size(2), target.size(3)\n",
    "    avg_pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    I_b = torch.norm(avg_pool(target) - torch.flip(avg_pool(target), dims=[3]), p=1, dim=1)\n",
    "    I_hat = torch.norm(avg_pool(output) - torch.flip(avg_pool(output), dims=[3]), p=1, dim=1)\n",
    "\n",
    "    loss = nn.L1Loss(reduction='sum')(I_b, I_hat) / (c * h * w)\n",
    "    return loss\n",
    "\n",
    "def w_smooth_loss(output, target):\n",
    "    c, h, w = target.size(1), target.size(2), target.size(3)\n",
    "    a_H = 1 - torch.norm(target[:, :, :-1, :] - target[:, :, 1:, :], p=1, dim=1)\n",
    "    a_W = 1 - torch.norm(target[:, :, :, :-1] - target[:, :, :, 1:], p=1, dim=1)\n",
    "    d_H = torch.norm(output[:, :, :-1, :] - output[:, :, 1:, :], p=1, dim=1)\n",
    "    d_W = torch.norm(output[:, :, :, :-1] - output[:, :, :, 1:], p=1, dim=1)\n",
    "\n",
    "    loss = nn.L1Loss(reduction='sum')(a_H * d_H + a_W * d_W) / (c * h * w)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m outputs_resized \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(outputs, size\u001b[38;5;241m=\u001b[39mclear_images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     98\u001b[0m pixel_loss_value \u001b[38;5;241m=\u001b[39m pixel_loss(outputs_resized, clear_images)\n\u001b[1;32m---> 99\u001b[0m perceptual_loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mperceptual_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_resized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclear_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvgg_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m lr_average_loss_value \u001b[38;5;241m=\u001b[39m lr_average_loss(outputs_resized, clear_images)\n\u001b[0;32m    101\u001b[0m w_smooth_loss_value \u001b[38;5;241m=\u001b[39m w_smooth_loss(outputs_resized, clear_images)\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mperceptual_loss\u001b[1;34m(output, target, vgg_model)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(output_features)):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(output_features[i])):\n\u001b[1;32m---> 17\u001b[0m         c_i_j, h_i_j, w_i_j \u001b[38;5;241m=\u001b[39m output_features[i][j]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[43moutput_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m, output_features[i][j]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     18\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)(output_features[i][j], target_features[i][j]) \u001b[38;5;241m/\u001b[39m (c_i_j \u001b[38;5;241m*\u001b[39m h_i_j \u001b[38;5;241m*\u001b[39m w_i_j)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class LaplacianPriorNet(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super(LaplacianPriorNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class SILPAutoencoder(nn.Module):\n",
    "    def __init__(self, out_channels, in_features, out_features):\n",
    "        super(SILPAutoencoder, self).__init__()\n",
    "        self.laplacian_prior_net = LaplacianPriorNet(out_channels)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # Calculate the size of the feature maps after the encoder\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, out_channels, 224, 224)\n",
    "            encoder_output_size = self.encoder(dummy_input).view(1, -1).size(1)\n",
    "        self.code_conversion = nn.Linear(encoder_output_size, out_features)  # Adjusted input size\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 3, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.upsample = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        laplacian_features = self.laplacian_prior_net(x)\n",
    "        encoded = self.encoder(laplacian_features)\n",
    "        # Flatten the encoder output before passing to the linear layer\n",
    "        code = self.code_conversion(encoded.view(encoded.size(0), -1))\n",
    "        decoded = self.decoder(code.view(code.size(0), -1, 1, 1))\n",
    "        outputs = self.upsample(decoded)\n",
    "        return outputs\n",
    "\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "\n",
    "# Set up the training process\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "out_channels = 128  # Example output channels of LaplacianPriorNet\n",
    "in_features = 224\n",
    "out_features = 128  # Example output dimension of decoder\n",
    "num_epochs = 10 \n",
    "\n",
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "occluded_dataset = CustomDataset(root_dir=r'C:\\Users\\dk\\Documents\\silp\\occluded\\masked', transform=transform)\n",
    "clear_dataset = CustomDataset(root_dir=r'C:\\Users\\dk\\Documents\\silp\\neutral', transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 32\n",
    "occluded_data_loader = DataLoader(occluded_dataset, batch_size=batch_size, shuffle=True)\n",
    "clear_data_loader = DataLoader(clear_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Assuming you have a pretrained VGG model\n",
    "vgg_model = vgg16(pretrained=True).features.to(device).eval() \n",
    "\n",
    "model = SILPAutoencoder(out_channels, in_features, out_features).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for occluded_images, clear_images in zip(occluded_data_loader, clear_data_loader):\n",
    "        occluded_images = occluded_images.to(device)\n",
    "        clear_images = clear_images.to(device)\n",
    "\n",
    "        # Calculate the losses\n",
    "        outputs = model(occluded_images)\n",
    "\n",
    "        # Resize outputs to match the size of clear_images\n",
    "        outputs_resized = torch.nn.functional.interpolate(outputs, size=clear_images.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        pixel_loss_value = pixel_loss(outputs_resized, clear_images)\n",
    "        perceptual_loss_value = perceptual_loss(outputs_resized, clear_images, vgg_model)\n",
    "        lr_average_loss_value = lr_average_loss(outputs_resized, clear_images)\n",
    "        w_smooth_loss_value = w_smooth_loss(outputs_resized, clear_images)\n",
    "        k1, k2, k3, k4 = 1, 0.25, 0.1, 0.1\n",
    "        total_loss = k1 * pixel_loss_value + k2 * perceptual_loss_value + k3 * lr_average_loss_value + k4 * w_smooth_loss_value\n",
    "\n",
    "        # Backpropagate and update the model\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_loss(output, target):\n",
    "    c, h, w = target.size(1), target.size(2), target.size(3)\n",
    "    loss = nn.MSELoss(reduction='sum')(output, target) / (c * h * w)\n",
    "    return loss\n",
    "def perceptual_loss(output, target, vgg_model):\n",
    "    output_features = vgg_model(output)\n",
    "    target_features = vgg_model(target)\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(len(output_features)):\n",
    "        for j in range(len(output_features[i])):\n",
    "            c_i_j, h_i_j, w_i_j = output_features[i][j].size(1), output_features[i][j].size(2), output_features[i][j].size(3)\n",
    "            loss += nn.L1Loss(reduction='sum')(output_features[i][j], target_features[i][j]) / (c_i_j * h_i_j * w_i_j)\n",
    "\n",
    "    return loss\n",
    "def lr_average_loss(output, target):\n",
    "    c, h, w = target.size(1), target.size(2), target.size(3)\n",
    "    avg_pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    I_b = torch.norm(avg_pool(target) - torch.flip(avg_pool(target), dims=[3]), p=1, dim=1)\n",
    "    I_hat = torch.norm(avg_pool(output) - torch.flip(avg_pool(output), dims=[3]), p=1, dim=1)\n",
    "\n",
    "    loss = nn.L1Loss(reduction='sum')(I_b, I_hat) / (c * h * w)\n",
    "    return loss\n",
    "def w_smooth_loss(output, target):\n",
    "    c, h, w = target.size(1), target.size(2), target.size(3)\n",
    "    a_H = 1 - torch.norm(target[:, :, :-1, :] - target[:, :, 1:, :], p=1, dim=1)\n",
    "    a_W = 1 - torch.norm(target[:, :, :, :-1] - target[:, :, :, 1:], p=1, dim=1)\n",
    "    d_H = torch.norm(output[:, :, :-1, :] - output[:, :, 1:, :], p=1, dim=1)\n",
    "    d_W = torch.norm(output[:, :, :, :-1] - output[:, :, :, 1:], p=1, dim=1)\n",
    "\n",
    "    loss = nn.L1Loss(reduction='sum')(a_H * d_H + a_W * d_W) / (c * h * w)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DIRECTORY = r\"dataset\"\n",
    "CATEGORIES = [\"with_mask\", \"without_mask\"]\n",
    "# Data loading and preprocessing\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a transformation to resize the images\n",
    "resize_transform = transforms.Resize((224, 224))\n",
    "\n",
    "# Data loading and preprocessing\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = Image.open(img_path)\n",
    "        image = resize_transform(image)  # Resize the image\n",
    "        image = transforms.ToTensor()(image)  # Convert to tensor\n",
    "\n",
    "        data.append(image)\n",
    "        labels.append(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "                                                  test_size=0.20, stratify=labels, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplacianPriorNet(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super(LaplacianPriorNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "class SILPAutoencoder(nn.Module):\n",
    "    def __init__(self, out_channels, in_features, out_features):\n",
    "        super(SILPAutoencoder, self).__init__()\n",
    "        self.laplacian_prior_net = LaplacianPriorNet(out_channels)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # Calculate the size of the feature maps after the encoder\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, out_channels, 224, 224)\n",
    "            encoder_output_size = self.encoder(dummy_input).view(1, -1).size(1)\n",
    "        self.code_conversion = nn.Linear(encoder_output_size, out_features)  # Adjusted input size\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 3, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.upsample = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        laplacian_features = self.laplacian_prior_net(x)\n",
    "        encoded = self.encoder(laplacian_features)\n",
    "        # Flatten the encoder output before passing to the linear layer\n",
    "        code = self.code_conversion(encoded.view(encoded.size(0), -1))\n",
    "        decoded = self.decoder(code.view(code.size(0), -1, 1, 1))\n",
    "        outputs = self.upsample(decoded)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
