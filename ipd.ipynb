{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def pixel_loss(output, target):\n",
    "    c, h, w = target.size(1), target.size(2), target.size(3)\n",
    "    loss = nn.MSELoss(reduction='sum')(output, target) / (c * h * w)\n",
    "    return loss\n",
    "\n",
    "def perceptual_loss(output, target, vgg_model):\n",
    "\n",
    "    output_features = vgg_model(output)\n",
    "    target_features = vgg_model(target)\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(len(output_features)):\n",
    "        for j in range(len(output_features[i])):\n",
    "            c_i_j, h_i_j, w_i_j = output_features[i][j].size(1), output_features[i][j].size(2), output_features[i][j].size(3)\n",
    "            loss += nn.L1Loss(reduction='sum')(output_features[i][j], target_features[i][j]) / (c_i_j * h_i_j * w_i_j)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def lr_average_loss(output, target):\n",
    "    \n",
    "    c, h, w = target.size(1), target.size(2), target.size(3)\n",
    "    avg_pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    I_b = torch.norm(avg_pool(target) - torch.flip(avg_pool(target), dims=[3]), p=1, dim=1)\n",
    "    I_hat = torch.norm(avg_pool(output) - torch.flip(avg_pool(output), dims=[3]), p=1, dim=1)\n",
    "\n",
    "    loss = nn.L1Loss(reduction='sum')(I_b, I_hat) / (c * h * w)\n",
    "    return loss\n",
    "\n",
    "def w_smooth_loss(output, target):\n",
    "    c, h, w = target.size(1), target.size(2), target.size(3)\n",
    "    a_H = 1 - torch.norm(target[:, :, :-1, :] - target[:, :, 1:, :], p=1, dim=1)\n",
    "    a_W = 1 - torch.norm(target[:, :, :, :-1] - target[:, :, :, 1:], p=1, dim=1)\n",
    "    d_H = torch.norm(output[:, :, :-1, :] - output[:, :, 1:, :], p=1, dim=1)\n",
    "    d_W = torch.norm(output[:, :, :, :-1] - output[:, :, :, 1:], p=1, dim=1)\n",
    "\n",
    "    loss = nn.L1Loss(reduction='sum')(a_H * d_H + a_W * d_W) / (c * h * w)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "class SILPAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels=1, hidden_dim=500):\n",
    "        super(SILPAutoencoder, self).__init__()\n",
    "        self.laplacian_prior = LaplacianPriorSubnetwork()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            LRMatchModule(64, 64),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            LRMatchModule(128, 128),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            LRMatchModule(256, 256),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.code_conversion = CodeConversionModule(256 * 8 * 8, hidden_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            LRMatchModule(256 + hidden_dim, 256, is_deconv=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "\n",
    "            LRMatchModule(128, 128, is_deconv=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "\n",
    "            LRMatchModule(64, 64, is_deconv=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        laplacian_prior = self.laplacian_prior(x)\n",
    "\n",
    "        encoded = self.encoder(x)\n",
    "        batch_size, channels, height, width = encoded.size()\n",
    "        code = self.code_conversion(encoded)\n",
    "        code = code.view(batch_size, -1, 1, 1)\n",
    "\n",
    "        combined_code = torch.cat((encoded, code), dim=1)\n",
    "        reconstructed = self.decoder(combined_code)\n",
    "\n",
    "        return reconstructed, laplacian_prior\n",
    "\n",
    "class CodeConversionModule(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim):\n",
    "        super(CodeConversionModule, self).__init__()\n",
    "        self.fc = nn.Linear(in_channels, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        x = x.view(batch_size, -1)  # Flatten the input\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
